{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aad01e2",
   "metadata": {},
   "source": [
    "# Tarea 1 - MEL\n",
    "\n",
    "Alejandro Mantilla - 201711304\n",
    "\n",
    "Ximena Palacio - 201730995\n",
    "\n",
    "Juan Manuel Betancourt - 201632544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ba593",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0620db",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "888a75fb",
   "metadata": {},
   "source": [
    "## Parte A. Problemas Teóricos y Conceptuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8c825",
   "metadata": {},
   "source": [
    "### Problema 1. Fundamentos de Inferencia Paramétrica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4895cd6d",
   "metadata": {},
   "source": [
    "Ayuda:\n",
    "* Si $Z \\sim \\text{Normal}(0, 1)$, entonces $Z^{2}\\sim \\chi^{2}(1)$.\n",
    "* Si $Z_{1} \\sim \\chi^{2}(v_{1})$ y $Z_{2} \\sim \\chi^{2}(v_{2})$ y  $Z_{1}$ y $Z_{2}$ son independientes, entonces $(Z_{1} + Z_{2}) \\sim\\chi^{2}(v_{1} + v_{2})$.\n",
    "* $\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}$ y $\\bar{Y}$ son independientes en el caso de $Y_{i} \\sim \\text{Normal}(\\mu,\\sigma^{2})$.\n",
    "\n",
    "Se tiene una muestra aleatoria $Y_1, Y_2, \\cdots , Y_n$ de una población $Y \\sim \\text{Normal}(0, 1)$. Demuestre:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e9468",
   "metadata": {},
   "source": [
    "__1.__ $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^{n}Y_{i} \\sim \\text{Normal}(0,1/n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c71a18",
   "metadata": {},
   "source": [
    "Partimos de que\n",
    "\n",
    "$$\n",
    "Y_{i}\\sim \\text{Normal}(0,1).\n",
    "$$\n",
    "\n",
    "Por propiedad de la varianza de la distribución normal, multiplicar la variable por un factor, afecta la distribución de manera que la varianza se multiplica por el cuadrado del factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8698b3",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\frac{Y_{i}}{n} &\\sim \\text{Normal}(0,1/{n^{2}})\\\\\n",
    "    \\frac{Y_{1}}{n} + \\frac{Y_{2}}{n} + \\cdots + \\frac{Y_{n}}{n} &\\sim \\text{Normal}(0,\\underbrace{1/n^{2} + 1/n^{2} + \\cdots + 1/n^{2}}_{n \\text{ veces}} )\\\\\n",
    "    \\sum_{i=1}^{n}\\frac{Y_{i}}{n} = \\frac{1}{n}\\sum_{i=1}^{n}Y_{i} &\\sim \\text{Normal}(0,\\sum_{i=1}^{n}1/{n^{2}})\\\\\n",
    "    \\frac{1}{n}\\sum_{i=1}^{n}Y_{i} &\\sim \\text{Normal}(0,\\frac{1}{n}\\sum_{i=1}^{n}1/{n})\\\\\n",
    "    \\frac{1}{n}\\sum_{i=1}^{n}Y_{i} &\\sim \\text{Normal}(0,1/n).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed87b203",
   "metadata": {},
   "source": [
    "__2.__ $\\sum_{i=1}^{n}Y_{i}^{2} \\sim \\chi^{2}(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c3f68",
   "metadata": {},
   "source": [
    "Demostraremos por inducción, dadas la primera y segunda ayuda.\n",
    "        \n",
    "__Paso base (__$n = 1$__):__\n",
    "\n",
    "$$\n",
    "    \\sum_{i = 1}^{1}Y_{i}^{2} = Y_{1}^{2} \\sim \\chi^{2}(1)\n",
    "$$\n",
    "\n",
    "La primera ayuda garantiza la afirmación.        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac4dea9",
   "metadata": {},
   "source": [
    "__Paso inductivo (suponer para__ $n = k$__, demostrar para__ $n = k+1$__):__\n",
    "        \n",
    "Suponemos que\n",
    "$$\n",
    "\\sum_{i=1}^{k}Y_{i}^{2} = Y_{1}^{2} + Y_{2}^{2} + \\cdots + Y_{k-1}^{2} + Y_{k}^{2} \\sim \\chi^{2}(k).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1c0a30",
   "metadata": {},
   "source": [
    "Ahora debemos demostrar que\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{k+1}Y_{i}^{2} \\sim \\chi^{2}(k+1).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc0e4c9",
   "metadata": {},
   "source": [
    "Como implicación del supuesto y del paso base, obtenemos que\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{k+1}Y_{i}^{2} = \\underbrace{\\left[Y_{1}^{2} + Y_{2}^{2} + \\cdots + Y_{k-1}^{2} + Y_{k}^{2}\\right]}_{\\sim \\chi^{2}(k)} + \\underbrace{\\left[Y_{k+1}^{2}\\right]}_{\\sim \\chi^{2}(1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c9947",
   "metadata": {},
   "source": [
    "Como implicación de la segunda ayuda, obtenemos que\n",
    "$$\n",
    "\\sum_{i=1}^{k+1}Y_{i}^{2} = \\underbrace{\\left[Y_{1}^{2} + Y_{2}^{2} + \\cdots + Y_{k-1}^{2} + Y_{k}^{2}\\right]}_{\\sim \\chi^{2}(k)} + \\underbrace{\\left[Y_{k+1}^{2}\\right]}_{\\sim \\chi^{2}(1)} \\sim \\chi^{2}((k) + (1)) = \\chi^{2}(k+1).\n",
    "$$\n",
    "        \n",
    "Con esto, hemos demostrado la afirmación inicial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c8309",
   "metadata": {},
   "source": [
    " __3.__ $\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2} \\sim \\chi^{2}(n-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee377e5",
   "metadata": {},
   "source": [
    "Por propiedad de la distribución normal y la primera ayuda, $\\bar{Y} \\sim \\text{Normal}\\left(0, \\frac{1}{n}\\right) \\Rightarrow \\frac{\\bar{Y}}{1/\\sqrt{n}} \\sim \\text{Normal}(0,1) \\Rightarrow \\left(\\frac{\\bar{Y}}{1/\\sqrt{n}}\\right)^{2} \\sim \\chi^{2}(1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca50479",
   "metadata": {},
   "source": [
    "Desarrollemos lo demostrado en el punto anterior, $\\sum_{i=1}^{n}Y_{i}^{2} \\sim \\chi^{2}(n)$.\n",
    "\\begin{align*}\n",
    "    \\sum_{i=1}^{n}Y_{i}^{2} &= \\sum_{i=1}^{n}\\left(Y_{i} - \\bar{Y} + \\bar{Y}\\right)^{2}\\\\\n",
    "    &= \\sum_{i=1}^{n}\\left(Y_{i} - \\bar{Y}\\right)^{2} + \\sum_{i=1}^{n}\\bar{Y}^{2} + 2\\bar{Y}\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right).\n",
    "\\end{align*}        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2177b5",
   "metadata": {},
   "source": [
    "Por propiedad de la muestra de la media, podemos ver que $\\sum_{i=1}^{n}Y_{i} = n\\bar{Y}$, por lo cual $\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right) = n\\bar{Y}-n\\bar{Y} = 0$.\n",
    "        \n",
    "Seguimos con el desarrollo:\n",
    "\\begin{align*}\n",
    "    \\sum_{i=1}^{n}Y_{i}^{2} &= \\sum_{i=1}^{n}\\left(Y_{i} - \\bar{Y}\\right)^{2} + \\sum_{i=1}^{n}\\bar{Y}^{2} + \\underbrace{2\\bar{Y}\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)}_{=0}\\\\\n",
    "    &= \\sum_{i=1}^{n}\\left(Y_{i} - \\bar{Y}\\right)^{2} + \\sum_{i=1}^{n}\\bar{Y}^{2}\\\\\n",
    "    &= \\sum_{i=1}^{n}\\left(Y_{i} - \\bar{Y}\\right)^{2} + n\\bar{Y}^{2}\\\\\n",
    "    &= \\sum_{i=1}^{n}\\left(Y_{i} - \\bar{Y}\\right)^{2} + \\underbrace{\\left(\\frac{\\bar{Y}}{1/\\sqrt{n}}\\right)^{2}}_{\\sim \\chi^{2}(1)}.\n",
    "\\end{align*}        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc4e95",
   "metadata": {},
   "source": [
    "Ahora despejamos para la suma original cuya distribución queremos conocer y aplicamos la indicación de la segunda ayuda.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\underbrace{\\sum_{i=1}^{n}Y_{i}^{2}}_{\\sim \\chi^{2}(n)} - \\underbrace{\\left(\\frac{\\bar{Y}}{1/\\sqrt{n}}\\right)^{2}}_{\\sim \\chi^{2}(1)} &= \\sum_{i=1}^{n}\\left(Y_{i} - \\bar{Y}\\right)^{2}\\\\\n",
    "    \\underbrace{\\sum_{i=1}^{n}Y_{i}^{2}}_{\\sim \\chi^{2}(n)} - \\underbrace{\\left(\\frac{\\bar{Y}}{1/\\sqrt{n}}\\right)^{2}}_{\\sim \\chi^{2}(1)} &= \\underbrace{\\sum_{i=1}^{n}\\left(Y_{i} - \\bar{Y}\\right)^{2}}_{\\sim \\chi^{2}(n-1)}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ebcdf",
   "metadata": {},
   "source": [
    "__4.__ $c \\cdot \\frac{n\\bar{Y}^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}} \\sim F(1, n-1)$; con $c = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de2df52",
   "metadata": {},
   "source": [
    "Partimos de la estructura de las variables aleatorias cuya distribución es $F(j, k)$.\n",
    "        \n",
    "Sean $W_{1}$ y $W_{2}$ variables aleatorias con distribución $\\chi^{2}$ con grados de libertad $j$ y $k$ respectivamente. Entonces,\n",
    "\n",
    "$$\n",
    "    \\frac{W_{1} / j}{W_{2} / k} \\sim F(j, k).\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd4b10",
   "metadata": {},
   "source": [
    "Sabemos de demostraciones anteriores que\n",
    "\\begin{align*}\n",
    "    n\\bar{Y}^{2} = \\left(\\frac{\\bar{Y}}{1/\\sqrt{n}}\\right)^{2} &\\sim \\chi^{2}(1)\\text{, y que}\\\\\n",
    "    \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2} &\\sim \\chi^{2}(n-1).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb58145",
   "metadata": {},
   "source": [
    "Haciendo uso de la tercera ayuda, debido a que el numerador y el denominador son independientes, podemos afirmar la siguiente relación:\n",
    "$$\n",
    "    \\frac{n\\bar{Y}^{2} / 1}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2} / (n-1)} \\sim F(1, n-1).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc2a76",
   "metadata": {},
   "source": [
    "Concluimos que\n",
    "$$\n",
    "    (n-1)\\cdot \\frac{n\\bar{Y}^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}} \\sim F(1, n-1)\n",
    "$$\n",
    "y por lo tanto $c=n-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ff15bb",
   "metadata": {},
   "source": [
    "### Problema 2. Fundamentos Distribuciones Multivariadas: Propiedades."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188040ab",
   "metadata": {},
   "source": [
    "Queremos demostrar que la matriz de covarianzas $Cov(\\hat{Y}, Y-\\hat{Y})$ contiene ceros únicamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b07ff2",
   "metadata": {},
   "source": [
    "Sabemos que la expresión matricial para la estimación de $Y$, es la siguiente:\n",
    "$$\n",
    "    \\hat{Y} = X\\left(X'X\\right)^{-1}X'Y = HY.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f88661",
   "metadata": {},
   "source": [
    "Podemos expresar $Y-\\hat{Y}$ como\n",
    "\\begin{align*}\n",
    "    Y-\\hat{Y} &= Y-HY\\\\\n",
    "              &= \\left(I - H\\right)Y.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa81860a",
   "metadata": {},
   "source": [
    "Utilizamos la ayuda del enunciado para expresar la matriz de covarianzas. Note que las matrices $H$ e $I$ son simétricas.\n",
    "\\begin{align*}\n",
    "    Cov(\\hat{Y}, Y-\\hat{Y}) &= \\,Cov(HY,\\left(I - H\\right)Y)\\\\\n",
    "                            &= H\\,Cov(Y,Y)\\left(I - H\\right)'\\\\\n",
    "                            &= H\\,Cov(Y,Y)\\left(I' - H'\\right)\\\\\n",
    "                            &= H\\,Cov(Y,Y)\\left(I - H\\right).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6ca75",
   "metadata": {},
   "source": [
    "El supuesto de que las observaciones son independientes entre sí nos permite que $Cov(Y,Y)=I$.\n",
    "\\begin{align*}\n",
    "    Cov(\\hat{Y}, Y-\\hat{Y}) &= H\\,Cov(Y,Y)\\left(I - H\\right)\\\\\n",
    "                            &= H\\,\\left(I - H\\right)\\\\\n",
    "                            &= H - H^{2}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb92b52",
   "metadata": {},
   "source": [
    "Por la indempotencia de $H$,\n",
    "\\begin{align*}\n",
    "    Cov(\\hat{Y}, Y-\\hat{Y}) &= H-H\\\\\n",
    "                            &= 0.\n",
    "\\end{align*}\n",
    "\n",
    "La matriz de covarianzas contiene solo ceros y por lo tanto $Y$ y $Y-\\hat{Y}$ son independientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba05a8",
   "metadata": {},
   "source": [
    "### Problema 3. Fundamentos de Inferencia Paramétrica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef56f3d6",
   "metadata": {},
   "source": [
    "\n",
    "Se tiene una muestra aleatoria $Y_1$, $Y_2$, $\\cdots$, $Y_n$ de una población $Y \\sim \\text{Normal}(\\mu, \\sigma^{2})$. El estimador convencional de la varianza es:\n",
    "$$\n",
    "    S^{2} = \\frac{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}}{n-1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27a1911",
   "metadata": {},
   "source": [
    "Sin embargo, el estimador por máxima verosimilitud es $\\hat{\\sigma}^{2}=\\frac{(n-1)S^{2}}{n}$. ¿Cuál de los dos es el mejor estimador? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8a7f9c",
   "metadata": {},
   "source": [
    "El estimador convencional de la varianza se puede reescribir de la siguiente forma:\n",
    "\\begin{align*}\n",
    "    S^{2} &= \\frac{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}}{n-1}\\\\\n",
    "          &= %\\frac{1}{n-1}(\\sum_{i=1}^{n}Y_{i}^{2}-n\\bar{Y}^{2}).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b96977",
   "metadata": {},
   "source": [
    "$$ S^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)\\left(Y_{i}-\\bar{Y}\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37f371",
   "metadata": {},
   "source": [
    "$$ S^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n}Y_{i} ^{2} + \\bar{Y}^{2} - 2Y_{i}\\bar{Y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e7579",
   "metadata": {},
   "source": [
    "$$ S^{2} = \\frac{1}{n-1} (\\sum_{i=1}^{n}Y_{i} ^{2} - 2Y_{i}\\bar{Y}) + n\\bar{Y}^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da246d46",
   "metadata": {},
   "source": [
    "En general, podemos expresar el ECM del estimador de un parámetro como:\n",
    "$$\n",
    "    \\text{ECM}(\\hat{\\theta}) = E(\\hat{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35326eac",
   "metadata": {},
   "source": [
    "Ahora bien, al calcular el ECM correspondiente a este estimador, se halla que:\n",
    "\\begin{align*}\n",
    "    ECM(S_{n-1}^{2}) &= %\\frac{1}{n}(\\mu_4-\\frac{n-3}{n-1}\\sigma^{4})\\\\\n",
    "    %&= \\frac{1}{n}(\\gamma_2+\\frac{2n}{n-1})\\sigma^{4}.\n",
    "\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c3b36d",
   "metadata": {},
   "source": [
    "### Problema 4. Fundamentos de Inferencia Paramétrica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a0b4ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72e957d9",
   "metadata": {},
   "source": [
    "### Problema 5. Fundamentos del Modelo de Regresión Lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edfcabf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb0adea1",
   "metadata": {},
   "source": [
    "### Problema 6. Fundamentos de Álgebra para Modelos Lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8594f8",
   "metadata": {},
   "source": [
    "Sean  $a$, $z$ en $\\mathbb{R}^p$ y $M$ una matriz en $\\mathbb{R}^{p \\times p}$. Se definen las funciones:\n",
    "\n",
    "$$\n",
    "\\alpha_1 = a^Tz \\ \\ \\ \\ \\ \\ \\ \\alpha_2 = z^T\\mathbb{M}z\n",
    "$$\n",
    "\n",
    "Demuestre:\n",
    "\n",
    "1. $\\frac{d\\alpha_1}{dz} = a$\n",
    "\n",
    "Para expresar la derivada de $\\alpha_1$ respecto al vector $z$ es necesario hallar la expresión resultante del producto. Al operar los dos vectores se tendría lo siguiente:\n",
    "\n",
    "$$\n",
    "a^Tz = \\begin{bmatrix}a_1 \\ \\  a_2 \\ \\ ... \\ \\  a_p \\end{bmatrix} \\begin{bmatrix}z_1 \\\\ z_2 \\\\ ... \\\\ z_p\\end{bmatrix} = a_1z_1 + a_2z_2+ ... + a_pz_p\n",
    "$$\n",
    "\n",
    "Al obtener la expresión esclar, es posible utilizar la definición del Jacobiano para hallar el vector de derivadas\n",
    "\n",
    "$$\n",
    "\\frac{d \\alpha_1}{z} = \\begin{bmatrix} \\frac{d \\alpha_1}{z_1} \\\\  \\frac{d \\alpha_1}{z_2} \\\\ ... \\\\ \\frac{d \\alpha_1}{z_p}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Es decir,\n",
    "\n",
    "$$\n",
    "\\frac{d \\alpha_1}{dz} = \\begin{bmatrix} \\frac{d}{dz_1} (a_1z_1 + a_2z_2+ ... + a_pz_p) \\\\  \\frac{d}{dz_2} (a_1z_1 + a_2z_2+ ... + a_pz_p) \\\\ ... \\\\ \\frac{d}{dz_p} (a_1z_1 + a_2z_2+ ... + a_pz_p) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "En cada componente $i$ del vector, la derivada será igual a $a_i$, dado que los demás términos serán constantes. Es decir, se obtendrá el siguiente vector.\n",
    "\n",
    "$$\n",
    "\\frac{d \\alpha_1}{dz} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ ... \\\\ a_p \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "De esta forma, es posible afirmar que \n",
    "\n",
    "$$\n",
    "\\frac{d \\alpha_1}{dz} = a\n",
    "$$\n",
    " \n",
    "\n",
    "2. $\\frac{d\\alpha_2}{dz} = (\\mathbb{M} + \\mathbb{M}^T)z$\n",
    "\n",
    "Nuevamente, para expresar la derivada de $\\alpha_2$ será necesario hallar la expresión escalar resultante del producto matricial. Esto, lo descompondremos en dos pasos. En primer lugar, el producto $z^T\\mathbb{M}$ se vería de la siguiente manera:\n",
    "\n",
    "$$\n",
    "z^T\\mathbb{M} = \\begin{bmatrix} z_1 \\ \\ z_2 \\ \\ ... \\ \\ z_p \\end{bmatrix} \\begin{bmatrix} m_{11} \\ \\ m_{12} \\ \\ ... \\ \\ m_{1p} \\\\ m_{21} \\ \\ m_{22} \\ \\ ... \\ \\ m_{2p} \\\\ ... \\ \\ ... \\ \\ ... \\ \\ ... \\\\ m_{p1} \\ \\ m_{p2} \\ \\ ... \\ \\ m_{pp}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Al observar la primera componenete del vector resultante, se vería la siguiente expresión:\n",
    "\n",
    "$$\n",
    "z_1m_{11} + z_2m_{21} + ... + z_pm_{p1}\n",
    "$$\n",
    "\n",
    "Consecuentemente, la expresión del i-ésimo componente de este vector será\n",
    "\n",
    "$$\n",
    "z_1m_{1i} + z_2m_{2i} + ... + z_pm_{pi}\n",
    "$$\n",
    "\n",
    "Esta expresión se puede compactar con una sumatoria. Es decir, el vector se puede ver como:\n",
    "\n",
    "$$\n",
    "z^T\\mathbb{M} = \\begin{bmatrix} \\sum_{i=1}^{p}{z_im_{i1}} \\ \\ \\ \\sum_{i=1}^{p}{z_im_{i2}} \\ \\ \\ ... \\ \\ \\ \\sum_{i=1}^{p}{z_im_{ip}} \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Para encontrar la expresión escalar, hace falta multiplicar el vector que se obtuvo por el vector $z$. Es decir\n",
    "\n",
    "$$\n",
    "z^T\\mathbb{M}z = \\begin{bmatrix} \\sum_{i=1}^{p}{z_im_{i1}} \\ \\ \\ \\sum_{i=1}^{p}{z_im_{i2}} \\ \\ \\ ... \\ \\ \\ \\sum_{i=1}^{p}{z_im_{ip}} \\end{bmatrix} \\begin{bmatrix}z_1 \\\\ z_2 \\\\ ... \\\\ z_p\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Así, la expresión resultante es:\n",
    "\n",
    "$$\n",
    "z^T\\mathbb{M}z = z_1\\sum_{i=1}^{p}{z_im_{i1}} + z_2\\sum_{i=1}^{p}{z_im_{i2}} + ... + z_p \\sum_{i=1}^{p}{z_im_{ip}}\n",
    "$$\n",
    "\n",
    "Una vez obtenemos la expresion escalar, empleamos el jacobiano para hallar la derivada de la función matricial. Para esto, veremos el primer componente del vector de derivadas, es decir $\\frac{d(z^T\\mathbb{M}z)}{dz_1}$. Para hallar la derivada, eliminaremos en primer lugar todos terminos que no contengan $z_1$ ya que serán constantes. Así, los terminos que quedan (sin derivar aún) son:\n",
    "\n",
    "$$\n",
    "z_1\\sum_{i=1}^{p}{z_im_{i1}} + z_2(z_1m_{12}) + ... + z_p(z_1m_{1p})\n",
    "$$\n",
    "\n",
    "La primera sumatoria se puede expandir para obtener lo siguiente:\n",
    "\n",
    "$$\n",
    "z_1(z_1m_{11} + z_2m_{21} + ... + z_pm_{p1}) + z_2(z_1m_{12}) + ... + z_p(z_1m_{1p})\n",
    "$$\n",
    "\n",
    "Al expandir la expresión se obtendría lo siguiente:\n",
    "\n",
    "$$\n",
    "(z_1z_1m_{11} + z_1z_2m_{21} + ... + z_1z_pm_{p1}) + z_2z_1m_{12} + ... + z_pz_1m_{1p}\n",
    "$$\n",
    "\n",
    "En este momento, se puede ver que la derivada sería la siguiente expresión:\n",
    "\n",
    "$$\n",
    "\\frac{d\\alpha_2}{dz_1} = (2z_1m_{11} + z_2m_{21} + ... + z_pm_{p1}) + z_2m_{12} + ... + z_pm_{1p} \\rightarrow \\frac{d\\alpha_2}{dz_1} = z_1(m_{11} + m_{11}) + z_2(m_{21} + m_{12}) + ... + z_p(m_{p1} + m_{1p})\\\n",
    "$$\n",
    "\n",
    "Al generalizar, la i-ésima derivada sería\n",
    "$$\n",
    "\\frac{d\\alpha_2}{dz_i} = z_1(m_{1i} + m_{i1}) + z_2(m_{2i} + m_{i2}) + ... + z_p(m_{pi} + m_{ip})\n",
    "$$\n",
    "\n",
    "Al organizar el vector de derivadas, es posible comprobar que es igual al producto $(\\mathbb{M} + \\mathbb{M}^T)z$\n",
    "\n",
    "$$\n",
    "\\frac{d\\alpha_2}{dz} = \\begin{bmatrix} z_1(m_{11} + m_{11}) + z_2(m_{21} + m_{12}) + ... + z_p(m_{p1} + m_{1p}) \\\\ z_1(m_{12} + m_{i2}) + z_2(m_{22} + m_{22}) + ... + z_p(m_{p2} + m_{2p}) \\\\ ... \\\\ z_1(m_{1p} + m_{p1}) + z_2(m_{2p} + m_{p2}) + ... + z_p(m_{pp} + m_{pp}) \\end{bmatrix} \n",
    "$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0037d3ba",
   "metadata": {},
   "source": [
    "### Problema 7. Fundamentos de Inferencia Paramétrica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3610d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b72debb7",
   "metadata": {},
   "source": [
    "### Problema 8. Proyecciones y Distribuciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061688ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c174ac1e",
   "metadata": {},
   "source": [
    "## Parte B. Problemas Aplicados con Datos Reales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1367666",
   "metadata": {},
   "source": [
    "### Problema 9. _Carseats Sales Data_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0bbfb5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6316f985",
   "metadata": {},
   "source": [
    "### Problema 10. Problema Libre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1307eb92",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

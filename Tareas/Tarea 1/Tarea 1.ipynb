{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aad01e2",
   "metadata": {},
   "source": [
    "# Tarea 1 - MEL\n",
    "\n",
    "Alejandro Mantilla - 201711304\n",
    "\n",
    "Ximena Palacio - 201730995\n",
    "\n",
    "Juan Manuel Betancourt - 201632544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ba593",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0620db",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca847e7b",
   "metadata": {},
   "source": [
    "## Índice\n",
    "\n",
    "- [Parte A](#Parte-A.-Problemas-Teóricos-y-Conceptuales.)\n",
    "    - [Problema 1](#Problema-1.-Fundamentos-de-Inferencia-Paramétrica.)\n",
    "        - [__I.__](#I.-$\\bar{Y}-=-\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}-\\sim-\\text{Normal}(0,1/n)$)\n",
    "        - [__II.__](#II.-$\\sum_{i=1}^{n}Y_{i}^{2}-\\sim-\\chi^{2}(n)$)\n",
    "        - [__III.__](#III.-$\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}-\\sim-\\chi^{2}(n-1)$)\n",
    "        - [__IV.__](#IV.-$c-\\cdot-\\frac{n\\bar{Y}^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}}-\\sim-F(1,-n-1)$;-con-$c-=-?$)\n",
    "    - [Problema 2](#Problema-2.-Fundamentos-Distribuciones-Multivariadas:-Propiedades.)\n",
    "    - [Problema 3](#Problema-3.-Fundamentos-de-Inferencia-Paramétrica.)\n",
    "    - [Problema 4](#Problema-4.-Fundamentos-de-Inferencia-Paramétrica.)\n",
    "        - [__I.__](#I.-Muestre-que-$\\sum_{i=1}^{n}a_{i}-=-1$.)\n",
    "        - [__II.__](#II.-Demuestre-que-de-todos-los-posibles-estimadores-insesgados-lineales-para-$\\mu$,-$\\bar{Y}$-es-el-que-tiene-menor-varianza-(BLUE).)\n",
    "    - [Problema 5](#Problema-5.-Fundamentos-del-Modelo-de-Regresión-Lineal.)\n",
    "        - [__I.__](#)\n",
    "        - [__II.__](#)\n",
    "        - [__III.__](#)\n",
    "        - [__IV.__](#)\n",
    "            - [__a.__](#)\n",
    "            - [__b.__](#)\n",
    "    - [Problema 6](#Problema-6.-Fundamentos-de-Álgebra-para-Modelos-Lineales.)\n",
    "        - [__I.__](#)\n",
    "        - [__II.__](#)\n",
    "    - [Problema 7](#Problema-7.-Fundamentos-de-Inferencia-Paramétrica.)\n",
    "        - [__I.__](#)\n",
    "        - [__II.__](#)\n",
    "        - [__III.__](#)\n",
    "        - [__IV.__](#)\n",
    "    - [Problema 8](#Problema-8.-Proyecciones-y-Distribuciones.)\n",
    "        - [__I.__](#)\n",
    "        - [__II.__](#)\n",
    "        - [__III.__](#)\n",
    "- [Parte B](#Parte-B.-Problemas-Aplicados-con-Datos-Reales.)\n",
    "    - [Problema 9](#Problema-9.-Carseats-Sales-Data.)\n",
    "        - [__I.__](#)\n",
    "        - [__II.__](#)\n",
    "        - [__III.__](#)\n",
    "        - [__IV.__](#)\n",
    "        - [__V.__](#)\n",
    "        - [__VI.__](#)\n",
    "        - [__VII.__](#)\n",
    "        - [__VIII.__](#)\n",
    "    - [Problema 10](#Problema-10.-Problema-Libre.)\n",
    "        - [__I.__](#)\n",
    "        - [__II.__](#)\n",
    "            - [__a.__](#)\n",
    "            - [__b.__](#)\n",
    "            - [__c.__](#)\n",
    "            - [__d.__](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a75fb",
   "metadata": {},
   "source": [
    "## Parte A. Problemas Teóricos y Conceptuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8c825",
   "metadata": {},
   "source": [
    "### Problema 1. Fundamentos de Inferencia Paramétrica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4895cd6d",
   "metadata": {},
   "source": [
    "Ayuda:\n",
    "* Si $Z \\sim \\text{Normal}(0, 1)$, entonces $Z^{2}\\sim \\chi^{2}(1)$.\n",
    "* Si $Z_{1} \\sim \\chi^{2}(v_{1})$ y $Z_{2} \\sim \\chi^{2}(v_{2})$ y  $Z_{1}$ y $Z_{2}$ son independientes, entonces $(Z_{1} + Z_{2}) \\sim\\chi^{2}(v_{1} + v_{2})$.\n",
    "* $\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}$ y $\\bar{Y}$ son independientes en el caso de $Y_{i} \\sim \\text{Normal}(\\mu,\\sigma^{2})$.\n",
    "\n",
    "Se tiene una muestra aleatoria $Y_1, Y_2, \\cdots , Y_n$ de una población $Y \\sim \\text{Normal}(0, 1)$. Demuestre:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e9468",
   "metadata": {},
   "source": [
    "#### __I.__ $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^{n}Y_{i} \\sim \\text{Normal}(0,1/n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a26a6",
   "metadata": {},
   "source": [
    "Partimos de que\n",
    "\n",
    "$$\n",
    "Y_{i}\\sim \\text{Normal}(0,1).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c71a18",
   "metadata": {},
   "source": [
    "Por propiedad de la varianza de la distribución normal, multiplicar la variable por un factor, afecta la distribución de manera que la varianza se multiplica por el cuadrado del factor.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{Y_{i}}{n} &\\sim \\text{Normal}(0,1/{n^{2}})\\\\\n",
    "    \\frac{Y_{1}}{n} + \\frac{Y_{2}}{n} + \\cdots + \\frac{Y_{n}}{n} &\\sim \\text{Normal}(0,\\underbrace{1/n^{2} + 1/n^{2} + \\cdots + 1/n^{2}}_{n \\text{ veces}} )\\\\\n",
    "    \\sum_{i=1}^{n}\\frac{Y_{i}}{n} = \\frac{1}{n}\\sum_{i=1}^{n}Y_{i} &\\sim \\text{Normal}(0,\\sum_{i=1}^{n}1/{n^{2}})\\\\\n",
    "    \\frac{1}{n}\\sum_{i=1}^{n}Y_{i} &\\sim \\text{Normal}(0,\\frac{1}{n}\\underbrace{\\sum_{i=1}^{n}1/{n}}_{=1})\\\\\n",
    "    \\frac{1}{n}\\sum_{i=1}^{n}Y_{i} &\\sim \\text{Normal}(0,1/n).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed87b203",
   "metadata": {},
   "source": [
    "#### __II.__ $\\sum_{i=1}^{n}Y_{i}^{2} \\sim \\chi^{2}(n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c3f68",
   "metadata": {},
   "source": [
    "Demostraremos por inducción, dadas la primera y segunda ayuda.\n",
    "        \n",
    "__Paso base (__$n = 1$__):__\n",
    "\n",
    "$$\n",
    "    \\sum_{i = 1}^{1}Y_{i}^{2} = Y_{1}^{2} \\sim \\chi^{2}(1)\n",
    "$$\n",
    "\n",
    "La primera ayuda garantiza la afirmación.        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac4dea9",
   "metadata": {},
   "source": [
    "__Paso inductivo (suponer para__ $n = k$__, demostrar para__ $n = k+1$__):__\n",
    "        \n",
    "Suponemos que\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{k}Y_{i}^{2} = Y_{1}^{2} + Y_{2}^{2} + \\cdots + Y_{k-1}^{2} + Y_{k}^{2} \\sim \\chi^{2}(k).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1c0a30",
   "metadata": {},
   "source": [
    "Ahora debemos demostrar que\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{k+1}Y_{i}^{2} \\sim \\chi^{2}(k+1).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc0e4c9",
   "metadata": {},
   "source": [
    "Como implicación del supuesto y del paso base, obtenemos que\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{k+1}Y_{i}^{2} = \\underbrace{\\left[Y_{1}^{2} + Y_{2}^{2} + \\cdots + Y_{k-1}^{2} + Y_{k}^{2}\\right]}_{\\sim \\chi^{2}(k)} + \\underbrace{\\left[Y_{k+1}^{2}\\right]}_{\\sim \\chi^{2}(1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c9947",
   "metadata": {},
   "source": [
    "Como implicación de la segunda ayuda, obtenemos que\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{k+1}Y_{i}^{2} = \\underbrace{\\left[Y_{1}^{2} + Y_{2}^{2} + \\cdots + Y_{k-1}^{2} + Y_{k}^{2}\\right]}_{\\sim \\chi^{2}(k)} + \\underbrace{\\left[Y_{k+1}^{2}\\right]}_{\\sim \\chi^{2}(1)} \\sim \\chi^{2}((k) + (1)) = \\chi^{2}(k+1).\n",
    "$$\n",
    "        \n",
    "Con esto, hemos demostrado la afirmación inicial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c8309",
   "metadata": {},
   "source": [
    " #### __III.__ $\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2} \\sim \\chi^{2}(n-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee377e5",
   "metadata": {},
   "source": [
    "Por propiedad de la distribución normal estándar y la primera ayuda, $\\bar{Y} \\sim \\text{Normal}\\left(0, \\frac{1}{n}\\right) \\Rightarrow \\frac{\\bar{Y}}{1/\\sqrt{n}} \\sim \\text{Normal}(0,1) \\Rightarrow \\left(\\frac{\\bar{Y}}{1/\\sqrt{n}}\\right)^{2} \\sim \\chi^{2}(1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca50479",
   "metadata": {},
   "source": [
    "Desarrollemos lo demostrado en el punto anterior, $\\sum_{i=1}^{n}Y_{i}^{2} \\sim \\chi^{2}(n)$.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sum_{i=1}^{n}Y_{i}^{2} &= \\sum_{i=1}^{n}\\left(Y_{i} - \\bar{Y} + \\bar{Y}\\right)^{2}\\\\\n",
    "    &= \\sum_{i=1}^{n}\\left(Y_{i} - \\bar{Y}\\right)^{2} + \\sum_{i=1}^{n}\\bar{Y}^{2} + 2\\bar{Y}\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right).\n",
    "\\end{align*}        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2177b5",
   "metadata": {},
   "source": [
    "Por propiedad de la muestra de la media, podemos ver que $\\sum_{i=1}^{n}Y_{i} = n\\bar{Y}$, por lo cual $\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right) = n\\bar{Y}-n\\bar{Y} = 0$.\n",
    "        \n",
    "Seguimos con el desarrollo:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sum_{i=1}^{n}Y_{i}^{2} &= \\sum_{i=1}^{n}\\left(Y_{i} - \\bar{Y}\\right)^{2} + \\sum_{i=1}^{n}\\bar{Y}^{2} + \\underbrace{2\\bar{Y}\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)}_{=0}\\\\\n",
    "    &= \\sum_{i=1}^{n}\\left(Y_{i} - \\bar{Y}\\right)^{2} + \\sum_{i=1}^{n}\\bar{Y}^{2}\\\\\n",
    "    &= \\sum_{i=1}^{n}\\left(Y_{i} - \\bar{Y}\\right)^{2} + n\\bar{Y}^{2}\\\\\n",
    "    &= \\sum_{i=1}^{n}\\left(Y_{i} - \\bar{Y}\\right)^{2} + \\underbrace{\\left(\\frac{\\bar{Y}}{1/\\sqrt{n}}\\right)^{2}}_{\\sim \\chi^{2}(1)}.\n",
    "\\end{align*}        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc4e95",
   "metadata": {},
   "source": [
    "Ahora despejamos para la suma original cuya distribución queremos conocer y aplicamos la indicación de la segunda ayuda:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\underbrace{\\underbrace{\\sum_{i=1}^{n}Y_{i}^{2}}_{\\sim \\chi^{2}(n)} - \\underbrace{\\left(\\frac{\\bar{Y}}{1/\\sqrt{n}}\\right)^{2}}_{\\sim \\chi^{2}(1)}}_{\\sim \\chi^{2}(n-1)} &= \\sum_{i=1}^{n}\\left(Y_{i} - \\bar{Y}\\right)^{2}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ebcdf",
   "metadata": {},
   "source": [
    "#### __IV.__ $c \\cdot \\frac{n\\bar{Y}^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}} \\sim F(1, n-1)$; con $c = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de2df52",
   "metadata": {},
   "source": [
    "Partimos de la estructura de las variables aleatorias cuya distribución es $F(j, k)$.\n",
    "        \n",
    "Sean $W_{1}$ y $W_{2}$ variables aleatorias con distribución $\\chi^{2}$ con grados de libertad $j$ y $k$ respectivamente. Entonces,\n",
    "\n",
    "$$\n",
    "    \\frac{W_{1} / j}{W_{2} / k} \\sim F(j, k).\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd4b10",
   "metadata": {},
   "source": [
    "Sabemos de demostraciones anteriores que\n",
    "\n",
    "\\begin{align*}\n",
    "    n\\bar{Y}^{2} = \\left(\\frac{\\bar{Y}}{1/\\sqrt{n}}\\right)^{2} &\\sim \\chi^{2}(1)\\text{, y que}\\\\\n",
    "    \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2} &\\sim \\chi^{2}(n-1).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb58145",
   "metadata": {},
   "source": [
    "Haciendo uso de la tercera ayuda, debido a que el numerador y el denominador son independientes, podemos afirmar la siguiente relación:\n",
    "\n",
    "$$\n",
    "    \\frac{n\\bar{Y}^{2} / 1}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2} / (n-1)} \\sim F(1, n-1).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc2a76",
   "metadata": {},
   "source": [
    "Concluimos que\n",
    "\n",
    "$$\n",
    "    (n-1)\\cdot \\frac{n\\bar{Y}^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}} \\sim F(1, n-1)\n",
    "$$\n",
    "\n",
    "y por lo tanto $c=n-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ff15bb",
   "metadata": {},
   "source": [
    "### Problema 2. Fundamentos Distribuciones Multivariadas: Propiedades."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188040ab",
   "metadata": {},
   "source": [
    "Queremos demostrar que la matriz de covarianzas $Cov(\\hat{Y}, Y-\\hat{Y})$ contiene ceros únicamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b07ff2",
   "metadata": {},
   "source": [
    "Sabemos que la expresión matricial para la estimación de $Y$, es la siguiente:\n",
    "\n",
    "$$\n",
    "    \\hat{Y} = X\\left(X'X\\right)^{-1}X'Y = HY.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f88661",
   "metadata": {},
   "source": [
    "Podemos expresar $Y-\\hat{Y}$ como\n",
    "\n",
    "\\begin{align*}\n",
    "    Y-\\hat{Y} &= Y-HY\\\\\n",
    "              &= \\left(I - H\\right)Y.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa81860a",
   "metadata": {},
   "source": [
    "Utilizamos la ayuda del enunciado para expresar la matriz de covarianzas. Note que las matrices $H$ e $I$ son simétricas.\n",
    "\n",
    "\\begin{align*}\n",
    "    Cov(\\hat{Y}, Y-\\hat{Y}) &= \\,Cov(HY,\\left(I - H\\right)Y)\\\\\n",
    "                            &= H\\,Cov(Y,Y)\\left(I - H\\right)'\\\\\n",
    "                            &= H\\,Cov(Y,Y)\\left(I' - H'\\right)\\\\\n",
    "                            &= H\\,Cov(Y,Y)\\left(I - H\\right).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6ca75",
   "metadata": {},
   "source": [
    "El supuesto de que las observaciones son independientes entre sí nos permite que $Cov(Y,Y)=I$.\n",
    "\n",
    "\\begin{align*}\n",
    "    Cov(\\hat{Y}, Y-\\hat{Y}) &= H\\,Cov(Y,Y)\\left(I - H\\right)\\\\\n",
    "                            &= H\\,\\left(I - H\\right)\\\\\n",
    "                            &= H - H^{2}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb92b52",
   "metadata": {},
   "source": [
    "Por la indempotencia de $H$,\n",
    "\n",
    "\\begin{align*}\n",
    "    Cov(\\hat{Y}, Y-\\hat{Y}) &= H-H\\\\\n",
    "                            &= 0.\n",
    "\\end{align*}\n",
    "\n",
    "La matriz de covarianzas contiene solo ceros y por lo tanto $Y$ y $Y-\\hat{Y}$ son independientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba05a8",
   "metadata": {},
   "source": [
    "### Problema 3. Fundamentos de Inferencia Paramétrica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef56f3d6",
   "metadata": {},
   "source": [
    "Se tiene una muestra aleatoria $Y_1$, $Y_2$, $\\cdots$, $Y_n$ de una población $Y \\sim \\text{Normal}(\\mu, \\sigma^{2})$. El estimador convencional de la varianza es:\n",
    "\n",
    "$$\n",
    "    S^{2} = \\frac{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}}{n-1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27a1911",
   "metadata": {},
   "source": [
    "Sin embargo, el estimador por máxima verosimilitud es $\\hat{\\sigma}^{2}=\\frac{(n-1)S^{2}}{n}$. ¿Cuál de los dos es el mejor estimador? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08846a52",
   "metadata": {},
   "source": [
    "Iniciamos el procedimiento, desarrollando la expresión del error cuadrático medio. En los siguientes pasos, $E$, se refiere al la función de valor esperado.\n",
    "\n",
    "\\begin{align*}\n",
    "    MSE\\left(\\hat{\\theta}\\right) &= E\\left[(\\hat{\\theta}-\\theta)^{2}\\right] \\\\\n",
    "    &= E\\left[(\\hat{\\theta} - \\theta)(\\hat{\\theta} - \\theta)\\right] \\\\\n",
    "    &= E\\left[\\hat{\\theta}^{2} + \\theta^{2} - 2\\theta\\hat{\\theta}\\right] \\\\\n",
    "    &= E\\left[\\hat{\\theta}^{2}\\right] + E\\left[\\theta^{2}\\right] - E\\left[2\\theta\\hat{\\theta}\\right] \\\\\n",
    "    &= E\\left[\\hat{\\theta}^{2}\\right] + \\theta^{2} - 2\\theta E \\left[\\hat{\\theta}\\right]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be359a70",
   "metadata": {},
   "source": [
    "Expresamos la varianza del estimador en terminos de su valor esperado y despejamos:\n",
    "\n",
    "\\begin{align*}\n",
    "    & Var\\left(\\hat{\\theta}\\right) = E\\left[\\hat{\\theta}^{2}\\right] - E\\left[\\hat{\\theta}\\right]^{2} \\\\\n",
    "    \\Rightarrow & E\\left[\\hat{\\theta}^{2}\\right] = Var\\left(\\hat{\\theta}\\right) + E\\left[\\hat{\\theta}\\right]^{2}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80728ecb",
   "metadata": {},
   "source": [
    "Expresamos el sesgo del estimador en terminos de su valor esperado:\n",
    "\n",
    "$$\n",
    "Bias\\left(\\hat{\\theta}\\right) = E\\left[\\hat{\\theta}\\right] - \\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3e599",
   "metadata": {},
   "source": [
    "Desarrollamos la expresión del $MSE$ según el sesgo y la varianza.\n",
    "\n",
    "\\begin{align*}\n",
    "    MSE\\left(\\hat{\\theta}\\right) &= E\\left[\\hat{\\theta}^{2}\\right] + \\theta^{2} - 2\\theta E \\left[\\hat{\\theta}\\right] \\\\\n",
    "    &= Var\\left(\\hat{\\theta}\\right) + E\\left[\\hat{\\theta}\\right]^{2} + \\theta^{2} - 2\\theta E \\left[\\hat{\\theta}\\right] \\\\\n",
    "    &= Var\\left(\\hat{\\theta}\\right) + \\left( E\\left[\\hat{\\theta}\\right] - \\theta\\right)^{2} \\\\\n",
    "    &= Var\\left(\\hat{\\theta}\\right) + \\left( Bias\\left(\\hat{\\theta}\\right) \\right)^{2}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc2d6b",
   "metadata": {},
   "source": [
    "A partir de la expresión dada para $S^{2}$ y de manera similar a la demostración del problema [1.3](#3.-$\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}-\\sim-\\chi^{2}(n-1)$):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06941853",
   "metadata": {},
   "source": [
    "Sea $X_{i} = Y_{i} - \\mu$. Podemos sustituir en $S^{2}$ los $Y_{i}$ por $X_{i}$ ya que restar una constante para cada uno de los $Y_{i}$ no cambia la medida de dispersión.\n",
    "\n",
    "\\begin{align*}\n",
    "    (n-1)S^{2} &= \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2} = \\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2} \\\\\n",
    "    &= \\sum_{i=1}^{n}(X_{i}^{2} - 2\\bar{X}X_{i} + \\bar{X}^{2}) \\\\\n",
    "    &= \\sum_{i=1}^{n}\\left(X_{i}^{2}\\right) - 2\\bar{X}\\sum_{i=1}^{n} \\left(X_{i}\\right) + n \\bar{X}^{2} \\\\\n",
    "    &= \\sum_{i=1}^{n}X_{i}^{2} - 2n\\bar{X}^{2} + n \\bar{X}^{2} \\\\\n",
    "    &= \\sum_{i=1}^{n} X_{i}^{2} - n\\bar{X}^{2} \\\\\n",
    "    \\frac{(n-1)S^{2}}{\\sigma^{2}} &= \\frac{\\sum_{i=1}^{n} X_{i}^{2}}{\\sigma^{2}} - \\frac{n\\bar{X}^{2}}{\\sigma^{2}} \\\\\n",
    "    &= \\sum_{i=1}^{n} \\left(\\frac{X_{i}}{\\sigma}\\right)^{2} - n\\left(\\frac{\\bar{X}}{\\sigma}\\right)^{2} \\\\\n",
    "    &= \\sum_{i=1}^{n} \\left(\\frac{X_{i}}{\\sigma}\\right)^{2} - \\left(\\frac{\\bar{X}}{\\sigma/\\sqrt{n}}\\right)^{2}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69de024",
   "metadata": {},
   "source": [
    "Notamos que $\\frac{X_{i}}{\\sigma}$ y $\\frac{\\bar{X}}{\\sigma/\\sqrt{n}}$ tienen distribución $\\text{Normal}(0,1)$ y que el lado derecho de la ecuación puede interpretarse como la suma de $n-1$ variables aleatorias al cuadrado. Como consecuencia de la ayuda 2 del [problema 1](#Problema-1.-Fundamentos-de-Inferencia-Paramétrica.),\n",
    "\n",
    "$$\n",
    "    \\frac{(n-1)S^{2}}{\\sigma^{2}} = \\underbrace{\\underbrace{\\sum_{i=1}^{n} \\left(\\frac{X_{i}}{\\sigma}\\right)^{2}}_{\\chi^{2}(n)} - \\underbrace{\\left(\\frac{\\bar{X}}{\\sigma/\\sqrt{n}}\\right)^{2}}_{\\chi^{2}(1)}}_{\\chi^{2}(n-1)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9953c",
   "metadata": {},
   "source": [
    "Por propiedades de la distribución $\\chi^{2}(n-1)$, calculamos la varianza de $S^{2}$:\n",
    "\n",
    "\\begin{align*}\n",
    "    Var\\left(\\frac{(n-1)S^{2}}{\\sigma^{2}}\\right) &= 2(n-1) \\\\\n",
    "    \\frac{(n-1)^{2}}{\\sigma^{4}}Var\\left(S^{2}\\right) &= 2(n-1) \\\\\n",
    "    Var\\left(S^{2}\\right) &= \\frac{2\\sigma^{4}}{n-1}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ae1ecd",
   "metadata": {},
   "source": [
    "Calculamos el valor esperado de $S^{2}$:\n",
    "\n",
    "\\begin{align*}\n",
    "    E\\left[\\frac{(n-1)S^{2}}{\\sigma^{2}}\\right] &= n-1 \\\\\n",
    "    \\frac{n-1}{\\sigma^{2}}E\\left[S^{2}\\right] &= n-1 \\\\\n",
    "    E\\left[S^{2}\\right] &= \\sigma^{2}. \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60044dad",
   "metadata": {},
   "source": [
    "Por consecuencia, $Bias\\left(S^{2}\\right) = 0$ y $MSE\\left( S^{2}\\right) = \\frac{2\\sigma^{4}}{n-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e09e9a3",
   "metadata": {},
   "source": [
    "Calculamos la varianza de $\\hat{\\sigma}^{2}$:\n",
    "\n",
    "\\begin{align*}\n",
    "    Var\\left(\\hat{\\sigma}^{2}\\right) &= Var\\left(\\frac{(n-1)S^{2}}{n}\\right) \\\\\n",
    "    &= \\left(\\frac{n-1}{n}\\right)^{2}Var\\left(S^{2}\\right) \\\\\n",
    "    &= \\left(\\frac{n-1}{n}\\right)^{2}\\frac{2\\sigma^{4}}{n-1} \\\\\n",
    "    &= \\frac{2\\sigma^{4} (n-1)}{n^{2}}. \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aab6ed1",
   "metadata": {},
   "source": [
    "Calculamos el valor esperado de $\\hat{\\sigma}^{2}$:\n",
    "\n",
    "\\begin{align*}\n",
    "    E\\left[\\hat{\\sigma}^{2}\\right] &= E\\left[\\frac{(n-1)S^{2}}{n}\\right] \\\\\n",
    "    &= \\frac{(n-1)}{n}E\\left[S^{2}\\right] \\\\\n",
    "    &= \\frac{(n-1)}{n}\\sigma^{2}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff983f6e",
   "metadata": {},
   "source": [
    "Por consecuencia, $Bias\\left(\\hat{\\sigma}^{2}\\right) = \\frac{(n-1)}{n}\\sigma^{2} - \\sigma^{2}$ y \n",
    "\n",
    "\\begin{align*}\n",
    "    MSE\\left( \\hat{\\sigma}^{2}\\right) &= \\frac{2\\sigma^{4} (n-1)}{n^{2}} + \\left( \\frac{(n-1)}{n}\\sigma^{2} - \\sigma^{2}\\right)^{2} \\\\\n",
    "    &= \\frac{2\\sigma^{4} (n-1)}{n^{2}} + \\sigma^{4}\\left( \\frac{(n-1)}{n} - 1\\right)^{2} \\\\\n",
    "    &= \\sigma^{4}\\left(\\frac{2 (n-1)}{n^{2}} + \\left( \\frac{(n-1)}{n} - 1\\right)^{2}\\right) \\\\\n",
    "    &= \\sigma^{4}\\left(\\frac{2n - 2}{n^{2}} + \\left( \\frac{-1}{n}\\right)^{2}\\right) \\\\\n",
    "    &= \\sigma^{4}\\left(\\frac{2n - 1}{n^{2}}\\right). \\\\\n",
    "    % &= \\sigma^{4}\\left(\\frac{2}{n} - \\frac{1}{n^{2}}\\right). \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1010282c",
   "metadata": {},
   "source": [
    "Dividimos los $MSE$ para identificar al mayor:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{MSE\\left( \\hat{\\sigma}^{2} \\right)}{MSE\\left( S^{2} \\right)} &= \\frac{\\frac{2\\sigma^{4}}{n-1}}{\\sigma^{4}\\left(\\frac{2n - 1}{n^{2}}\\right)} \\\\\n",
    "    &= \\frac{2\\sigma^{4}}{(n-1)\\sigma^{4}\\left(\\frac{2n - 1}{n^{2}}\\right)} \\\\\n",
    "    &= \\frac{2n^{2}}{(n-1)(2n - 1)} \\\\\n",
    "    &= \\frac{2n^{2}}{2n^{2} -3n + 1}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f600f7f",
   "metadata": {},
   "source": [
    "Resulta evidente que $\\frac{MSE\\left( \\hat{\\sigma}^{2} \\right)}{MSE\\left( S^{2} \\right)} > 1, \\ \\forall n \\geq 2$, lo que nos permite concluir que $S^{2}$ tiene mejor $MSE$ y, por lo menos para este criterio, es mejor estimador.\n",
    "\n",
    "Claro está que existen distintos estimadores de la varianza que minimizan el $MSE$, mejor de lo que lo hace $S^{2}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c3b36d",
   "metadata": {},
   "source": [
    "### Problema 4. Fundamentos de Inferencia Paramétrica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a0b4ee",
   "metadata": {},
   "source": [
    "Se tiene una muestra aleatoria $Y_1, Y_2, \\cdots, Y_n$ de una población $Y \\sim f_{Y}(y)$. Se sabe además que\n",
    "$\\mu = E\\left[Y\\right]$ y $\\sigma^{2} = Var\\left(Y\\right)$ son dos parámetros desconocidos (media y varianza de la población). Se define un estimador lineal insesgado de $\\mu$ como:\n",
    "\n",
    "$$\n",
    "    \\hat{\\mu} = \\sum_{i=1}^{n}a_{i}Y_{i},\n",
    "$$\n",
    "\n",
    "donde $a_{1}, \\cdots,a_{n}$ son constantes arbitrarias y $E[\\hat{\\mu}] = \\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7c5396",
   "metadata": {},
   "source": [
    "#### __I.__ Muestre que $\\sum_{i=1}^{n}a_{i} = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a098aa1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6ee2cf3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c84bc4e4",
   "metadata": {},
   "source": [
    "#### __II.__ Demuestre que de todos los posibles estimadores insesgados lineales para $\\mu$, $\\bar{Y}$ es el que tiene menor varianza (BLUE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b4e77",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6872b08",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72e957d9",
   "metadata": {},
   "source": [
    "### Problema 5. Fundamentos del Modelo de Regresión Lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edfcabf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb0adea1",
   "metadata": {},
   "source": [
    "### Problema 6. Fundamentos de Álgebra para Modelos Lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8594f8",
   "metadata": {},
   "source": [
    "Sean  $a$, $z$ en $\\mathbb{R}^p$ y $M$ una matriz en $\\mathbb{R}^{p \\times p}$. Se definen las funciones:\n",
    "\n",
    "$$\n",
    "\\alpha_1 = a^Tz \\ \\ \\ \\ \\ \\ \\ \\alpha_2 = z^T\\mathbb{M}z\n",
    "$$\n",
    "\n",
    "Demuestre:\n",
    "\n",
    "1. $\\frac{d\\alpha_1}{dz} = a$\n",
    "\n",
    "Para expresar la derivada de $\\alpha_1$ respecto al vector $z$ es necesario hallar la expresión resultante del producto. Al operar los dos vectores se tendría lo siguiente:\n",
    "\n",
    "$$\n",
    "a^Tz = \\begin{bmatrix}a_1 \\ \\  a_2 \\ \\ ... \\ \\  a_p \\end{bmatrix} \\begin{bmatrix}z_1 \\\\ z_2 \\\\ ... \\\\ z_p\\end{bmatrix} = a_1z_1 + a_2z_2+ ... + a_pz_p\n",
    "$$\n",
    "\n",
    "Al obtener la expresión esclar, es posible utilizar la definición del Jacobiano para hallar el vector de derivadas\n",
    "\n",
    "$$\n",
    "\\frac{d \\alpha_1}{z} = \\begin{bmatrix} \\frac{d \\alpha_1}{z_1} \\\\  \\frac{d \\alpha_1}{z_2} \\\\ ... \\\\ \\frac{d \\alpha_1}{z_p}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Es decir,\n",
    "\n",
    "$$\n",
    "\\frac{d \\alpha_1}{dz} = \\begin{bmatrix} \\frac{d}{dz_1} (a_1z_1 + a_2z_2+ ... + a_pz_p) \\\\  \\frac{d}{dz_2} (a_1z_1 + a_2z_2+ ... + a_pz_p) \\\\ ... \\\\ \\frac{d}{dz_p} (a_1z_1 + a_2z_2+ ... + a_pz_p) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "En cada componente $i$ del vector, la derivada será igual a $a_i$, dado que los demás términos serán constantes. Es decir, se obtendrá el siguiente vector.\n",
    "\n",
    "$$\n",
    "\\frac{d \\alpha_1}{dz} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ ... \\\\ a_p \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "De esta forma, es posible afirmar que \n",
    "\n",
    "$$\n",
    "\\frac{d \\alpha_1}{dz} = a\n",
    "$$\n",
    " \n",
    "\n",
    "2. $\\frac{d\\alpha_2}{dz} = (\\mathbb{M} + \\mathbb{M}^T)z$\n",
    "\n",
    "Nuevamente, para expresar la derivada de $\\alpha_2$ será necesario hallar la expresión escalar resultante del producto matricial. Esto, lo descompondremos en dos pasos. En primer lugar, el producto $z^T\\mathbb{M}$ se vería de la siguiente manera:\n",
    "\n",
    "$$\n",
    "z^T\\mathbb{M} = \\begin{bmatrix} z_1 \\ \\ z_2 \\ \\ ... \\ \\ z_p \\end{bmatrix} \\begin{bmatrix} m_{11} \\ \\ m_{12} \\ \\ ... \\ \\ m_{1p} \\\\ m_{21} \\ \\ m_{22} \\ \\ ... \\ \\ m_{2p} \\\\ ... \\ \\ ... \\ \\ ... \\ \\ ... \\\\ m_{p1} \\ \\ m_{p2} \\ \\ ... \\ \\ m_{pp}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Al observar la primera componenete del vector resultante, se vería la siguiente expresión:\n",
    "\n",
    "$$\n",
    "z_1m_{11} + z_2m_{21} + ... + z_pm_{p1}\n",
    "$$\n",
    "\n",
    "Consecuentemente, la expresión del i-ésimo componente de este vector será\n",
    "\n",
    "$$\n",
    "z_1m_{1i} + z_2m_{2i} + ... + z_pm_{pi}\n",
    "$$\n",
    "\n",
    "Esta expresión se puede compactar con una sumatoria. Es decir, el vector se puede ver como:\n",
    "\n",
    "$$\n",
    "z^T\\mathbb{M} = \\begin{bmatrix} \\sum_{i=1}^{p}{z_im_{i1}} \\ \\ \\ \\sum_{i=1}^{p}{z_im_{i2}} \\ \\ \\ ... \\ \\ \\ \\sum_{i=1}^{p}{z_im_{ip}} \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Para encontrar la expresión escalar, hace falta multiplicar el vector que se obtuvo por el vector $z$. Es decir\n",
    "\n",
    "$$\n",
    "z^T\\mathbb{M}z = \\begin{bmatrix} \\sum_{i=1}^{p}{z_im_{i1}} \\ \\ \\ \\sum_{i=1}^{p}{z_im_{i2}} \\ \\ \\ ... \\ \\ \\ \\sum_{i=1}^{p}{z_im_{ip}} \\end{bmatrix} \\begin{bmatrix}z_1 \\\\ z_2 \\\\ ... \\\\ z_p\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Así, la expresión resultante es:\n",
    "\n",
    "$$\n",
    "z^T\\mathbb{M}z = z_1\\sum_{i=1}^{p}{z_im_{i1}} + z_2\\sum_{i=1}^{p}{z_im_{i2}} + ... + z_p \\sum_{i=1}^{p}{z_im_{ip}}\n",
    "$$\n",
    "\n",
    "Una vez obtenemos la expresion escalar, empleamos el jacobiano para hallar la derivada de la función matricial. Para esto, veremos el primer componente del vector de derivadas, es decir $\\frac{d(z^T\\mathbb{M}z)}{dz_1}$. Para hallar la derivada, eliminaremos en primer lugar todos terminos que no contengan $z_1$ ya que serán constantes. Así, los terminos que quedan (sin derivar aún) son:\n",
    "\n",
    "$$\n",
    "z_1\\sum_{i=1}^{p}{z_im_{i1}} + z_2(z_1m_{12}) + ... + z_p(z_1m_{1p})\n",
    "$$\n",
    "\n",
    "La primera sumatoria se puede expandir para obtener lo siguiente:\n",
    "\n",
    "$$\n",
    "z_1(z_1m_{11} + z_2m_{21} + ... + z_pm_{p1}) + z_2(z_1m_{12}) + ... + z_p(z_1m_{1p})\n",
    "$$\n",
    "\n",
    "Al expandir la expresión se obtendría lo siguiente:\n",
    "\n",
    "$$\n",
    "(z_1z_1m_{11} + z_1z_2m_{21} + ... + z_1z_pm_{p1}) + z_2z_1m_{12} + ... + z_pz_1m_{1p}\n",
    "$$\n",
    "\n",
    "En este momento, se puede ver que la derivada sería la siguiente expresión:\n",
    "\n",
    "$$\n",
    "\\frac{d\\alpha_2}{dz_1} = (2z_1m_{11} + z_2m_{21} + ... + z_pm_{p1}) + z_2m_{12} + ... + z_pm_{1p} \\rightarrow \\frac{d\\alpha_2}{dz_1} = z_1(m_{11} + m_{11}) + z_2(m_{21} + m_{12}) + ... + z_p(m_{p1} + m_{1p})\\\n",
    "$$\n",
    "\n",
    "Al generalizar, la i-ésima derivada sería\n",
    "$$\n",
    "\\frac{d\\alpha_2}{dz_i} = z_1(m_{1i} + m_{i1}) + z_2(m_{2i} + m_{i2}) + ... + z_p(m_{pi} + m_{ip})\n",
    "$$\n",
    "\n",
    "Al organizar el vector de derivadas, es posible comprobar que es igual al producto $(\\mathbb{M} + \\mathbb{M}^T)z$\n",
    "\n",
    "$$\n",
    "\\frac{d\\alpha_2}{dz} = \\begin{bmatrix} z_1(m_{11} + m_{11}) + z_2(m_{21} + m_{12}) + ... + z_p(m_{p1} + m_{1p}) \\\\ z_1(m_{12} + m_{i2}) + z_2(m_{22} + m_{22}) + ... + z_p(m_{p2} + m_{2p}) \\\\ ... \\\\ z_1(m_{1p} + m_{p1}) + z_2(m_{2p} + m_{p2}) + ... + z_p(m_{pp} + m_{pp}) \\end{bmatrix} \n",
    "$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0037d3ba",
   "metadata": {},
   "source": [
    "### Problema 7. Fundamentos de Inferencia Paramétrica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3610d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b72debb7",
   "metadata": {},
   "source": [
    "### Problema 8. Proyecciones y Distribuciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061688ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c174ac1e",
   "metadata": {},
   "source": [
    "## Parte B. Problemas Aplicados con Datos Reales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1367666",
   "metadata": {},
   "source": [
    "### Problema 9. _Carseats Sales Data_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0bbfb5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6316f985",
   "metadata": {},
   "source": [
    "### Problema 10. Problema Libre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1307eb92",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
